{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse, gdal\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model, load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Land Cover Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Background                  </td><td style=\"text-align: right;\">0</td></tr>\n",
       "<tr><td>Buildings                   </td><td style=\"text-align: right;\">1</td></tr>\n",
       "<tr><td>Roads/Parking Lots/Driveways</td><td style=\"text-align: right;\">2</td></tr>\n",
       "<tr><td>Planted/Darker Cropland     </td><td style=\"text-align: right;\">3</td></tr>\n",
       "<tr><td>Water                       </td><td style=\"text-align: right;\">4</td></tr>\n",
       "<tr><td>Forest                      </td><td style=\"text-align: right;\">5</td></tr>\n",
       "<tr><td>Harvested/Open/Bare         </td><td style=\"text-align: right;\">6</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [[\"Background\", 0],\n",
    "         [\"Buildings\", 1],\n",
    "         [\"Roads/Parking Lots/Driveways\", 2],\n",
    "         [\"Planted/Darker Cropland\", 3],\n",
    "         [\"Water\", 4],\n",
    "         [\"Forest\", 5],\n",
    "         [\"Harvested/Open/Bare\", 6]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt = 'html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Land Cover Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = 7\n",
    "\n",
    "color_map = [\n",
    "    [52, 52, 52],\n",
    "    [76, 230, 0],\n",
    "    [255, 255, 115],\n",
    "    [255, 190, 232],\n",
    "    [204, 204, 204],\n",
    "    [0, 92, 230],\n",
    "    [100, 100, 100]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = np.array([0.000001, 2, 1, 1, 1, 2, 2])\n",
    "weights       = K.variable(class_weights)\n",
    "\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    # scale predictions so that the class probas of each sample sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis = -1, keepdims = True)\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calculate loss and weight loss\n",
    "    loss = y_true * K.log(y_pred) * weights\n",
    "    loss = -K.sum(loss, -1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save GeoTIFF Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image_data, path, geo_transform, projection):\n",
    "    \n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    # Set Info of Image\n",
    "    height, width = image_data.shape\n",
    "    dataset       = driver.Create(path, width, height, 1, gdal.GDT_Byte)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    dataset.GetRasterBand(1).WriteArray(image_data)\n",
    "\n",
    "    # Create Color Table\n",
    "    color_table = gdal.ColorTable()\n",
    "    \n",
    "    for i in range(number_of_classes):\n",
    "        color_table.SetColorEntry(i, tuple(color_map[i]) + (255,))\n",
    "    dataset.GetRasterBand(1).SetRasterColorTable(color_table)\n",
    "    dataset.GetRasterBand(1).SetNoDataValue(255)\n",
    "\n",
    "    dataset.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Model to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_image(input_image_path, model, output_image_path):\n",
    "    \n",
    "    input_dataset = gdal.Open(input_image_path)\n",
    "    input_image   = input_dataset.ReadAsArray().astype(np.float32)\n",
    "    # print(input_image.shape)\n",
    "    input_image = np.rollaxis(input_image, 0, 3)\n",
    "    h, w, n     = input_image.shape\n",
    "    # print(input_image.shape)\n",
    "    \n",
    "    model_input_height, model_input_width, model_input_channels    = model.layers[0].input_shape[1:4]\n",
    "    # print(model_input_height, model_input_width, model_input_channels)\n",
    "    model_output_height, model_output_width, model_output_channels = model.layers[len(model.layers) - 1].output_shape[1:4]\n",
    "    # print(model_output_height, model_output_width, model_output_channels)\n",
    "\n",
    "    padding_y = int((model_input_height - model_output_height)/2)\n",
    "    padding_x = int((model_input_width - model_output_width)/2)\n",
    "    assert model_output_channels == number_of_classes\n",
    "\n",
    "    pred_lc_image = np.zeros((h, w, number_of_classes))\n",
    "    # print(pred_lc_image.shape)\n",
    "    mask = np.ones((h, w))\n",
    "    # print(mask.shape)\n",
    "\n",
    "    irows, icols = [],[]\n",
    "    batch_size   = 16\n",
    "    minibatch    = []\n",
    "    ibatch       = 0\n",
    "    mb_array     = np.zeros((batch_size, model_input_width, model_input_height, model_input_channels))\n",
    "    # print(mb_array.shape)\n",
    "\n",
    "    n_rows = int(h / model_output_height)\n",
    "    # print(n_rows)\n",
    "    n_cols = int(w / model_output_width)\n",
    "    # print(n_cols)\n",
    "    \n",
    "    for row_idx in tqdm(range(n_rows)):\n",
    "        for col_idx in range(n_cols):\n",
    "            \n",
    "            subimage = input_image[row_idx*model_output_height:row_idx*model_output_height + model_input_height,\n",
    "                                   col_idx*model_output_width:col_idx*model_output_width + model_input_width, :] / 256.0\n",
    "            # print(subimage.shape)\n",
    "            \n",
    "            if(subimage.shape == model.layers[0].input_shape[1:4]):\n",
    "                \n",
    "                mb_array[ibatch] = subimage\n",
    "                ibatch += 1\n",
    "                irows.append((row_idx*model_output_height + padding_y,row_idx*model_output_height + model_input_height - padding_y))\n",
    "                icols.append((col_idx*model_output_width +  padding_x,col_idx*model_output_width  + model_input_width  - padding_x))\n",
    "\n",
    "                if (ibatch) == batch_size:\n",
    "\n",
    "                    outputs = model.predict(mb_array)\n",
    "                    for i in range(batch_size):\n",
    "                        r0,r1 = irows[i]\n",
    "                        c0,c1 = icols[i]\n",
    "\n",
    "                        pred_lc_image[r0:r1, c0:c1, :] = outputs[i]\n",
    "                        mask[r0:r1, c0:c1] = 0\n",
    "\n",
    "                    ibatch = 0\n",
    "                    irows,icols = [],[]\n",
    "\n",
    "    if ibatch > 0:\n",
    "        outputs = model.predict(mb_array)\n",
    "        for i in range(ibatch):\n",
    "            r0,r1 = irows[i]\n",
    "            c0,c1 = icols[i]\n",
    "\n",
    "            pred_lc_image[r0:r1, c0:c1, :] = outputs[i]\n",
    "            mask[r0:r1, c0:c1] = 0\n",
    "\n",
    "\n",
    "    label_image = np.ma.array(pred_lc_image.argmax(axis=-1), mask = mask)\n",
    "    save_image(label_image.filled(255), output_image_path, input_dataset.GetGeoTransform(), input_dataset.GetProjection())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Images in Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_dir, model_path, output_dir):\n",
    "\n",
    "    model = load_model(model_path, \n",
    "                       custom_objects = {'mean_iou': mean_iou, 'weighted_categorical_crossentropy': weighted_categorical_crossentropy})\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        if not files: continue\n",
    "\n",
    "        for f in files:\n",
    "            pth     = os.path.join(root,f)\n",
    "            out_pth = os.path.join(output_dir,f.split('.')[0]+'_C.tif')\n",
    "            eval_image(pth, model, out_pth)\n",
    "            print('saved result to ' + out_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_dir  = r'D:\\land_cover\\data\\raw\\image'\n",
    "model            = r'D:\\land_cover\\models\\model_epochs_05.h5'\n",
    "output_image_dir = r'D:\\land_cover\\data\\results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Script Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:15<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3008718_ne_16_1_20171024_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:12<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3008718_nw_16_1_20171024_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:12<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3008718_se_16_1_20171024_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:12<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3008718_sw_16_1_20171024_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3408601_ne_16_1_20170711_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:12<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3408601_nw_16_1_20170711_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3408601_se_16_1_20170711_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:16<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3408601_sw_16_1_20170711_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:12<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3408602_ne_16_1_20170711_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:12<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3408602_nw_16_1_20170711_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:11<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3408602_se_16_1_20170711_C.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:12<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to D:\\land_cover\\data\\results\\m_3408602_sw_16_1_20170711_C.tif\n"
     ]
    }
   ],
   "source": [
    "evaluate(input_image_dir, model, output_image_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geoai_dl)",
   "language": "python",
   "name": "geoai_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
